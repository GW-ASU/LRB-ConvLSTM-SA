{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qlongyan.ASURITE\\Miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "'''Evaluate Function'''\n",
    "def NSE(obs, sim):\n",
    "    obs, sim = obs.squeeze(), sim.squeeze()\n",
    "    a=((obs-sim)**2).sum()\n",
    "    c=obs.mean()\n",
    "    b=((obs-c)**2).sum()\n",
    "    return 1-a/b\n",
    "\n",
    "def RMSE(obs, sim):\n",
    "    return np.sqrt(((obs-sim)**2).mean())\n",
    "\n",
    "def MSE(obs, sim):\n",
    "    return ((obs-sim)**2).mean()\n",
    "\n",
    "def PBIAS(obs, sim):\n",
    "    a=(sim-obs).sum()\n",
    "    b=obs.sum()\n",
    "    return 100*a/b\n",
    "\n",
    "def KGE(y_true, y_pred):\n",
    "    r = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    beta = np.sum(y_pred) / np.sum(y_true)\n",
    "    gamma = np.std(y_pred) / np.std(y_true)\n",
    "    kge = 1 - np.sqrt((r - 1)**2 + (beta - 1)**2 + (gamma - 1)**2)\n",
    "    return kge\n",
    "\n",
    "#---------------------------------\n",
    "'''ConvLSTM Module'''\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channel: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_channel: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, hidden_channel, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.hidden_channel = hidden_channel\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.in_channel+self.hidden_channel,\n",
    "                              out_channels=4*self.hidden_channel,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, inputs, states):\n",
    "        h, c = states\n",
    "        \n",
    "        combined = torch.cat([inputs, h], dim=1)  # concatenate along channel axis\n",
    "        \n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_channel, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_channel, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_channel, height, width, device=self.conv.weight.device))\n",
    "    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        try:\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, val=0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "#--------------------------------\n",
    "'''Spatial Attention'''\n",
    "class ChannelMaxPool(nn.Module):\n",
    "    def __init__(self, kernel_size, **kwargs):\n",
    "        super(ChannelMaxPool, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        n, c, w, h = inputs.size()\n",
    "        inputs = inputs.view(n, c, w * h).permute(0, 2, 1)\n",
    "        pooled = F.max_pool1d(inputs, self.kernel_size)\n",
    "        _, _, c = pooled.size()\n",
    "        pooled = pooled.permute(0, 2, 1)\n",
    "        return pooled.view(n, c, w, h)\n",
    "\n",
    "class ChannelAvgPool(nn.Module):\n",
    "    def __init__(self, kernel_size, **kwargs):\n",
    "        super(ChannelAvgPool, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        n, c, w, h = inputs.size()\n",
    "        inputs = inputs.view(n, c, w * h).permute(0, 2, 1)\n",
    "        pooled = F.avg_pool1d(inputs, self.kernel_size)\n",
    "        _, _, c = pooled.size()\n",
    "        pooled = pooled.permute(0, 2, 1)\n",
    "        return pooled.view(n, c, w, h)\n",
    "\n",
    "class ConvAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Attention (Woo et al, 2018). see https://arxiv.org/pdf/1807.06521.pdf\n",
    "    \n",
    "    idea: \n",
    "        F' = Ms(F) * F <-- spatial attention\n",
    "        \n",
    "        where\n",
    "        the spatial attention weight is\n",
    "            Ms(F) = sigmoid(f_{7x7}([AvgPool_c(F); MaxPool_c(F)])) --> shape: (N, 1, h, w)\n",
    "    \n",
    "    input: \n",
    "        h_t shape: (N, C, h, w) --> ht shape: (N, C, h*w)\n",
    "        x_t shape: (N, 1, h, w)\n",
    "    \n",
    "    after spatial attention: \n",
    "    element-wise multiplication gives new values with the shape of (N, C, h, w)\n",
    "    therefore, use dot product\n",
    "    \n",
    "    output: the redistributed ht (N, 1, C)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_filter, dropout, **kwargs):\n",
    "        super(ConvAttention, self).__init__(**kwargs)\n",
    "        # nn.Linear Applies a linear transformation to the incoming data, xA.T+b,\n",
    "        # where x has the shape of (N, in_dim), A has the shape of (out_dim, in_dim),\n",
    "        # and b has the shape of (out_dim,), therefore, \n",
    "        # the output should have the shape of (N, out_dim)\n",
    "        \n",
    "        # nn.matmul: mat1 (N, N, M), mat2 (N, M, P) --> output (N, N, P)\n",
    "        self.avg_pool = ChannelAvgPool(kernel_size=num_filter)\n",
    "        self.max_pool = ChannelMaxPool(kernel_size=num_filter)\n",
    "        self.conv_7x7 = nn.Conv2d(in_channels=5,\n",
    "                                  out_channels=1,\n",
    "                                  kernel_size=7,\n",
    "                                  padding=3,\n",
    "                                  bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, xt, ht, ct):\n",
    "        ht_avg = self.avg_pool(ht) # (N, 1, h, w)\n",
    "        ht_max = self.max_pool(ht) # (N, 1, h, w)\n",
    "        \n",
    "        ct_avg = self.avg_pool(ct) # (N, 1, h, w)\n",
    "        ct_max = self.max_pool(ct) # (N, 1, h, w)\n",
    "        \n",
    "        \"\"\"use xt, ct_avg, ct_max, ht_avg, ht_max.\"\"\"\n",
    "        ht_cat = torch.cat([xt, ct_avg, ct_max, ht_avg, ht_max], axis=1) # (N, 5, h, w)\n",
    "        \n",
    "        #===================================\n",
    "        '''use softmax function.'''\n",
    "\n",
    "        #-----------------------------------\n",
    "        #\"\"\"this is the version for no constraint on the study area\"\"\"\n",
    "        self.attention_weights = F.softmax(self.conv_7x7(ht_cat).flatten(start_dim=2), dim=-1) # (N, 1, h*w)\n",
    "        values_new = ht * self.attention_weights.reshape(ht_avg.shape) # (N, C, h, w)\n",
    "        values_new = values_new.sum(axis=1, keepdim=True)\n",
    "        #-----------------------------------\n",
    "\n",
    "        #===================================\n",
    "        \n",
    "        return values_new, self.attention_weights.reshape(ht_avg.shape), [ct_avg, ct_max, ht_avg, ht_max]\n",
    "\n",
    "#------------------------------------------------\n",
    "'''Model building'''\n",
    "class ConvLSTM_attn(nn.Module):\n",
    "    def __init__(self, num_filter, in_channel, dropout):\n",
    "        super(ConvLSTM_attn, self).__init__()\n",
    "        self.convlstm = ConvLSTMCell(in_channel=in_channel,\n",
    "                                     hidden_channel=num_filter,\n",
    "                                     kernel_size=(3, 3),\n",
    "                                     bias=True)\n",
    "        \n",
    "        self.attn_space = ConvAttention(num_filter=num_filter, dropout=dropout)\n",
    "\n",
    "    def roll(self, seq_len, x, h_t, c_t):\n",
    "        hs, cs = [], []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.convlstm(inputs=x[:, :, t, :, :],\n",
    "                                     states=[h_t, c_t])\n",
    "            hs += [h_t]\n",
    "            cs += [c_t]\n",
    "        \n",
    "        return hs, cs\n",
    "\n",
    "    def forward(self, x, states=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs:\n",
    "            5-D Tensor of shape (b, c, t, h, w)        #   batch, channel, time, height, width\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, _, seq_len, h, w = x.size()\n",
    "        \n",
    "        h_t, c_t = self.convlstm.init_hidden(batch_size=batch_size, image_size=(h, w)) if states == None else states\n",
    "        \n",
    "        hs, cs = self.roll(seq_len, x, h_t, c_t)\n",
    "        # hs[t] should have the shape of (N, C, h, w)\n",
    "        \n",
    "        # the output of attention should be (N, 1, h, w)\n",
    "        weighted_focus_saved = [(self.attn_space(x[:, :, t, :, :], hs[t], cs[t])) for t in range(len(hs))]\n",
    "        gridded_runoff = [weighted_focus_saved[t][0] for t in range(len(hs))]\n",
    "        attention_weights = [weighted_focus_saved[t][1] for t in range(len(hs))]\n",
    "        states_stored = [weighted_focus_saved[t][2] for t in range(len(hs))]\n",
    "        \n",
    "        weighted_focus = [gridded_runoff[t].sum(axis=-1).sum(axis=-1) for t in range(len(hs))]\n",
    "        outputs = torch.stack([weighted_focus[t] for t in range(len(hs))], axis=1)\n",
    "        \n",
    "        return outputs, torch.stack(gridded_runoff, axis=1), torch.stack(attention_weights, axis=1), states_stored\n",
    "\n",
    "#------------------------------------------------\n",
    "'''Model without attention'''\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, num_filter, in_channel, dropout):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.convlstm0 = ConvLSTMCell(in_channel=in_channel,\n",
    "                                     hidden_channel=num_filter,\n",
    "                                     kernel_size=(3, 3),\n",
    "                                     bias=True)\n",
    "        self.pool = nn.AvgPool2d((2, 2), stride=(2, 2))\n",
    "        self.fc = nn.Linear(in_features=num_filter*14*9, out_features=1, bias=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def roll(self, seq_len, x, h_t, c_t):\n",
    "        hs, cs = [], []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.convlstm0(inputs=x[:, :, t, :, :],\n",
    "                                        states=[h_t, c_t])\n",
    "            c_t = self.dropout(c_t)\n",
    "            \n",
    "            hs += [self.pool(h_t)]\n",
    "            cs += [self.pool(c_t)]\n",
    "        \n",
    "        return h_t, c_t, hs, cs\n",
    "\n",
    "    def forward(self, x, states=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs:\n",
    "            5-D Tensor of shape (b, c, t, h, w)        #   batch, channel, time, height, width\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, _, seq_len, h, w = x.size()\n",
    "        \n",
    "        h_t, c_t = self.convlstm0.init_hidden(batch_size=batch_size, image_size=(h, w)) if states == None else states\n",
    "        \n",
    "        h_t, c_t, hs, _ = self.roll(seq_len, x, h_t, c_t)\n",
    "        outputs = torch.stack([self.dropout(self.fc(hs[i].flatten(start_dim=1))) for i in range(len(hs))], axis=1)\n",
    "        outputs = F.relu(outputs)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "#------------------------------------------------\n",
    "def loadData(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "def wrapData(data_range, batch_size, DEVICE, is_train, dp_save):\n",
    "    '''Wrap the datasest as data iter.'''\n",
    "    start_pre_dataset = time.time()\n",
    "    Xs, ys = [], []\n",
    "    \n",
    "    for k in data_range:\n",
    "        file = np.load(dp_save+'LoganRiver_'+str(k)+'.npz')\n",
    "        Xs.append(torch.Tensor(file['X']))\n",
    "        ys.append(torch.Tensor(file['y']))\n",
    "\n",
    "    features, labels = torch.stack(Xs, axis=0).to(DEVICE), torch.stack(ys, axis=0).to(DEVICE)\n",
    "    data_iter = loadData((features, labels), batch_size, is_train=is_train)\n",
    "    print('Time cost %.3fs during preparing the data iter' %(time.time()-start_pre_dataset))\n",
    "    return data_iter\n",
    "\n",
    "def validate(net, data_iter, loss):\n",
    "    \"\"\"Evaluate the loss for a model on a dataset during the training process.\"\"\"\n",
    "    net.eval()\n",
    "    ls, n = .0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            out, _, _, _ = net(X.unsqueeze(1))\n",
    "            l = loss(out, y)\n",
    "            ls += l.detach().cpu().numpy()\n",
    "            n += len(out.reshape(-1))\n",
    "    return ls/n\n",
    "\n",
    "def evaluate(net, data_iter):\n",
    "    \"\"\"Return the prediction after the training process.\"\"\"\n",
    "    net.eval()\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    sims, obss = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            out, _, _, _ = net(X.unsqueeze(1))\n",
    "            sims.append(out.squeeze().detach().cpu().numpy())\n",
    "            obss.append(y.squeeze().detach().cpu().numpy())\n",
    "    print('Time cost %.3fs in total' %(time.time() - start))\n",
    "    return obss, sims\n",
    "\n",
    "def train(net, trainer, loss, train_iter, val_iter, num_epochs): \n",
    "    '''\n",
    "    train the model.\n",
    "    '''\n",
    "    start_all = time.time()\n",
    "    net.train()\n",
    "    train_ls_all_epoch, val_ls_all_epoch = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        ls = .0\n",
    "        n = 0\n",
    "        for X, y in train_iter:\n",
    "            out, _, _, _ = net(X.unsqueeze(1))\n",
    "            l = loss(out, y)\n",
    "            trainer.zero_grad()\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            ls += l.detach().cpu().numpy()\n",
    "            n += len(out.reshape(-1))\n",
    "        train_ls_all_epoch.append(ls/n)\n",
    "        val_ls = validate(net, val_iter, loss)\n",
    "        val_ls_all_epoch.append(val_ls)\n",
    "        print('Epoch %d: train loss %.6e, test loss %.6e, time cost %.3fs' %(epoch+1, ls/n, val_ls, time.time()-start))\n",
    "    print('Time cost %.3fs total' %(time.time()-start_all))\n",
    "    return train_ls_all_epoch, val_ls_all_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0], [1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 16\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# data_range = [i for i in range(9)]\n",
    "# train_range = random.sample(data_range, 8)\n",
    "# train_rest = [i for i in data_range if i not in train_range]\n",
    "# val_range = random.sample(train_rest, 1)\n",
    "# test_range = [9, 10, 11, 12]\n",
    "\n",
    "# We prepare the very long sequence for the whole train, validation and test period here, \n",
    "# to accelerate the training process, the data chuck can be divived to more pieces\n",
    "train_range = [0]\n",
    "val_range = [1]\n",
    "test_range = [2]\n",
    "\n",
    "train_range, val_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost 0.077s during preparing the data iter\n",
      "Time cost 0.012s during preparing the data iter\n",
      "total number of parameters: 15445\n",
      "torch.Size([80, 21, 3, 3])\n",
      "torch.Size([80])\n",
      "torch.Size([1, 5, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "dp_save = '../data/data_packed/'\n",
    "\n",
    "batch_size = 8\n",
    "lr = 1e-3\n",
    "num_epochs = 3\n",
    "\n",
    "train_iter = wrapData(train_range, batch_size, DEVICE, is_train=True, dp_save=dp_save)\n",
    "val_iter = wrapData(val_range, batch_size, DEVICE, is_train=True, dp_save=dp_save)\n",
    "# test_iter = wrapData(test_range, batch_size, DEVICE, is_train=False, dp_save=dp_save)\n",
    "#%%------------------------------------------------------------------------\n",
    "\n",
    "'''With attention'''\n",
    "net = ConvLSTM_attn(num_filter=20, in_channel=1, dropout=.2)\n",
    "\n",
    "'''Without attention'''\n",
    "# net = ConvLSTM(num_filter=20, in_channel=1, dropout=.2)\n",
    "\n",
    "#%%------------------------------------------------------------------------\n",
    "'''train from scratch'''\n",
    "net.to(DEVICE)\n",
    "net.apply(init_weights)\n",
    "\n",
    "#%%------------------------------------------------------------------------\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('total number of parameters:', pytorch_total_params)\n",
    "\n",
    "#%%------------------------------------------------------------------------\n",
    "loss = nn.MSELoss()\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "for p in net.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 5.184682e-06, test loss 3.392755e-05, time cost 38.020s\n",
      "Epoch 2: train loss 3.754370e-06, test loss 2.494710e-05, time cost 50.158s\n",
      "Epoch 3: train loss 2.723071e-06, test loss 1.901933e-05, time cost 54.103s\n",
      "Time cost 142.281s total\n"
     ]
    }
   ],
   "source": [
    "train_ls_all_epoch, val_ls_all_epoch = train(net, trainer, loss, train_iter, val_iter, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_ls_all_epoch, label='train')\n",
    "plt.plot(val_ls_all_epoch, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
